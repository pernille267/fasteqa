# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Estimate bias and skewness corrected bootstrap confidence intervals
#' 
#' @title Estimate bias and skewness corrected bootstrap confidence intervals
#' @name BCa_bootstrap_ci
#' 
#' @param bootstrapped_parameter_estimates Numeric vector - Bootstrapped estimates of the parameter of interest
#' @param jackknife_parameter_estimates Numeric vector - Jack knife estimates of the parameter of interest
#' @param original_parameter_estimate Double - Original point estimate of the parameter of interest
#' @param level Double - Required confidence level for the estimated confidence interval
#' @param silence Integer - Controls the progress reports outputted for debugging and further examination of the command. \code{silence = -1} or \code{silence = 0} signify that progress reports should be printed. Default is \code{silence = 1} which suppresses all printing
#'
#' @description Estimation of bootstrap confidence intervals for a parameter of interest using the BCa-variant
#'
#' @details To get an adequate estimate of the theoretical confidence interval, it is essential that original data represents the underlying distribution for the population parameter. More data typically corresponds with larger probability of our data being representative. For bootstrap confidence interval estimation independent of jack knife estimates, \code{bootstrap_ci()} may be used instead
#'
#' @return A numeric vector. The first element of the vector is the lower bound of the confidence interval, and the second element of the vector is the upper bound of the confidence interval 
#'
#' @examples \dontrun{
#'   fictional_original_estimate <- 2.1
#'   fictional_bootstrap_estimates <- runif(n = 1e3, min = 0.5, max = 2.5)
#'   fictional_jackknife_estimates <- runif(n = 1e1, min = 1.5, max = 2.5)
#'   BCa_bootstrap_ci(fictional_bootstrap_estimates,
#'                    fictional_jackknife_estimates,
#'                    fictional_original_estimate,
#'                    level = 0.95,
#'                    silence = 1)
#' }
NULL

BCa_bootstrap_ci <- function(bootstrapped_parameter_estimates, jackknife_parameter_estimates, original_parameter_estimate = 0, level = 0.95, silence = 1L) {
    .Call(`_fasteqa_BCa_bootstrap_ci`, bootstrapped_parameter_estimates, jackknife_parameter_estimates, original_parameter_estimate, level, silence)
}

#' Estimate simple bootstrap confidence intervals
#' 
#' @title Estimate simple bootstrap confidence intervals
#' @name bootstrap_ci
#' 
#' @param bootstrapped_parameter_estimates Numeric vector - Bootstrapped estimates of the parameter of interest
#' @param original_parameter_estimate Double - Original point estimate of the parameter of interest
#' @param type Integer - Which type of bootstrap confidence interval is required
#' \itemize{
#'   \item{\code{1: }}{Standard normal bootstrap confidence interval}
#'   \item{\code{2: }}{Basic bootstrap confidence interval}
#'   \item{\code{3: }}{Percentile bootstrap confidence interval}
#' }
#' @param level Double - Required confidence level for the estimated confidence interval
#' @param silence Integer - Controls the progress reports outputted for debugging and further examination of the command. \code{silence = -1} or \code{silence = 0} signify that progress reports should be printed. Default is \code{silence = 1} which suppresses all printing
#'
#' @description Estimation of bootstrap confidence intervals for a parameter of interest
#'
#' @details To get an adequate estimate of the theoretical confidence interval, it is essential that original data represents the underlying distribution for the population parameter. More data typically corresponds with larger probability of our data being representative. For second-order accurate and transformation respective confidence intervals, you may employ \code{BCa_bootstrap_ci()} instead. Note that \code{BCa_bootstrap_ci()} require jack knife estimates of the parameter of interest as well as bootstrap estimates 
#'
#' @return A numeric vector. The first element of the vector is the lower bound of the confidence interval, and the second element of the vector is the upper bound of the confidence interval 
#'
#' @examples \dontrun{
#'   fictional_original_estimate <- 2.1
#'   fictional_bootstrap_estimates <- runif(n = 1e3, min = 0.5, max = 2.5)
#'   bootstrap_ci(fictional_bootstrap_estimates,
#'                fictional_original_estimate,
#'                type = 3,
#'                level = 0.95,
#'                silence = 1)
#' }
NULL

bootstrap_ci <- function(bootstrapped_parameter_estimates, original_parameter_estimate = 0, type = 3L, level = 0.95, silence = 1L) {
    .Call(`_fasteqa_bootstrap_ci`, bootstrapped_parameter_estimates, original_parameter_estimate, type, level, silence)
}

#' Estimate differences in non-selectivity with zeta
#' 
#' @title Estimate differences in non-selectivity with zeta
#' @name estimate_zeta
#' @param data \code{list} or \code{data table} - Data with elements/columns \code{SampleID}, \code{ReplicateID}, \code{MP_A} and \code{MP_B}
#' @param silence \code{integer} - How much progress reports should be returned. Note that returning progress reports will slow down the performance drastically. There are three valid inputs:
#' \itemize{
#'   \item{\code{1: }}{All progress reports are silenced and is the default}
#'   \item{\code{0: }}{Some progress reports are delivered, but debugging reports are suppressed}
#'   \item{\code{-1: }}{All prorgress reports are delivered}
#' }
#' 
#' @description Estimate the degree of differences in non-selectivity with zeta. Zeta is is the ratio of the pooled average prediction error variance and the sum of analytical variances.  
#' 
#' @details Differences in non-selectivity between measurement systems may cause problems in e.g., evaluation of commutability. A large value of zeta indicates that we have have large differences in non-selectivity between compared measurement systems. An upper limit of acceptable zeta may be determined based on the allowable increase in prediction interval width and analyte of relevance
#' 
#' @return A list with the point estimate of zeta. The zeta value is a float value, meaning that the precision is 1e-6 (six decimals precision).
#'
#' @examples \dontrun{
#'   library(fasteqa)
#'   data <- simulate_data_eqa(list(n = 25, R = 3, cvx = 0.06, cvy = 0.04))
#'   estimate_zeta(data)
#' }
#'
NULL

estimate_zeta <- function(data, silence = 1L) {
    .Call(`_fasteqa_estimate_zeta`, data, silence)
}

#' Apply a mathematical summary function on every SampleID
#' 
#' @title Apply a mathematical summary function on every SampleID
#' @name fun_of_replicates
#' 
#' @param data \code{list} or \code{data table} - Data with elements/columns \code{SampleID}, \code{ReplicateID}, \code{MP_A} and \code{MP_B}
#' @param fun \code{string} - Used to choose the summary function one want to apply on each samples' measurements. Here is the list of possible valid summary functions:
#' \itemize{
#'   \item{\code{mean: }}{Taking the average of replicated measurments for samples with at least one measurement. Default}
#'   \item{\code{median: }}{Taking the median of replicated measurments for samples with at least one measurement}
#'   \item{\code{min: }}{Taking the minimum of replicated measurments for samples with at least one measurement}
#'   \item{\code{max: }}{Taking the maximum of replicated measurements for samples with at least one measurement}
#'   \item{\code{var: }}{Taking the variance of replicated measurements for samples with at least two measurements}
#'   \item{\code{sd: }}{Taking the standard deviation of replicated measurements for samples with at least two measurements}
#'   \item{\code{cv: }}{Taking the coefficient of variation of replicated measurements for samples with at least two measurements}
#' }
#' @param silence \code{integer} - Should debugging messages be printed. Default is no
#'
#' @description A practical function to evaluate summary typical summary functions of samples' replicated measurements. Taking mean of replicates, sd of replicates or cv of replicates are typical when analyzing EQA data so these are very useful. The remaning functions are not used as much, but may be needed in some cases so they are included based on this fact. In order for this function to work propery you need to ensure that:
#' \itemize{
#'   \item{\code{SampleID: }}{Must be a character vector}
#'   \item{\code{ReplicateID: }}{Must be a character vector}
#'   \item{\code{MP_A: }}{Must be a numeric vector}
#'   \item{\code{MP_B: }}{Must be a numeric vector}
#' }
#'
#' @details The difference between this function and \code{mean_of_replicates()} method in the \code{commutability.selectivity}, is that this is more than ten times faster.
#'
#' @return list - data containing the elements that is needed to build the resulting data of results. Use \code{setDT()} for maximum efficiency when converting the list into a data table 
#'
#' @examples \dontrun{
#'   library(fasteqa)
#'   data <- simulate_eqa_data()
#'   mean_of_replicates_data <- fun_of_replicates(data)
#'   var_of_replicates_data <- fun_of_replicates(data, "var")
#' }
NULL

fun_of_replicates <- function(data, fun = "mean", silence = 1L) {
    .Call(`_fasteqa_fun_of_replicates`, data, fun, silence)
}

#' Calculate imprecision point estimates of measurements in a given MS comparison 
#' 
#' @title Calculate imprecision point estimates of measurements in a given MS comparison
#' @name global_precision_estimates
#' 
#' @param data \code{list} or \code{data table} - Data with elements/columns \code{SampleID}, \code{ReplicateID}, \code{MP_A} and \code{MP_B}
#' @param silence \code{integer} - Should debugging messages be printed. Default is no
#'
#' @description Structure-requirement of \code{data}:
#' \itemize{
#'   \item{\code{SampleID: }}{Must be a character vector}
#'   \item{\code{ReplicateID: }}{Must be a character vector}
#'   \item{\code{MP_A: }}{Must be a numeric vector}
#'   \item{\code{MP_B: }}{Must be a numeric vector}
#' }
#'
#' @details Calculates various global imprecision estimates. To get CVs in percent you need only to multiply the raw CV estimates with 100. Here is a rough explaination of the output list:
#' \itemize{
#'   \item{\code{Var_A: }}{Pooled variance of all sample-variances based on MS_A}
#'   \item{\code{Var_B: }}{Pooled variance of all sample-variances based on MS_B}
#'   \item{\code{CV_A: }}{Global CV estimate based on Var_A and the grand mean of all measurements from MS_A}
#'   \item{\code{CV_B: }}{Global CV estimate based on Var_B and the grand mean of all measurements from MS_B}
#'   \item{\code{lambda: }}{Ratio of pooled variances Var_A and Var_B}
#' }
#' 
#'
#' @return \code{list} - with point imprecision estimates \code{Var_A}, \code{Var_B}, \code{CV_A}, \code{CV_B} and \code{lambda}
#' @examples \dontrun{
#'   library(fasteqa)
#'   data <- simulate_eqa_data(list(n = 25, R = 3, cvx = 0.02, cvy = 0.3))
#'   data$SampleID <- as.character(data$SampleID)
#'   data$ReplicateID <- as.character(data$ReplicateID)
#'   global_prcision_estimates(data = data)
#' }
NULL

global_precision_estimates <- function(data, silence = 1L) {
    .Call(`_fasteqa_global_precision_estimates`, data, silence)
}

#' Leave-one-out on clustered EQA clinical sample data
#' 
#' @title Leave-one-out on clustered EQA clinical sample data
#' @name leave_one_out
#' 
#' @param data \code{List} or \code{data table} - Data with list elements or data table columns with names \code{SampleID}, \code{ReplicateID}, \code{MP_A} and \code{MP_B}. \code{SampleID} and \code{ReplicateID} must be of character type for the function to operate correctly
#' @param loo_id Integer - Which of the samples of \code{SampleID} should be left out. Default value is 1
#'
#' @description Needed to calculate jack knife estimates of a parameter, that is required when using \code{BCa_bootstrap_ci()}. Alternatively one could use the Jack knife estimates to calculate standard error or bias of the estimator of relevance
#'
#' @details loo_ids can not be vectorized directly in R. Use \code{sapply()} or \code{replicate()} to leave out sample IDs one by one 
#'
#' @return A \code{list} containing the original data, but without the sample id corresponding to the given \code{loo_id} 
#'
#' @examples \dontrun{
#'   library(commutability.selectivity)
#'   data <- sdwdnsp2()
#'   loo_data <- leave_one_out(data, loo_id = 5)
#' }
NULL

leave_one_out <- function(data, loo_id = 1L) {
    .Call(`_fasteqa_leave_one_out`, data, loo_id)
}

#' Merge all computations efficiently into one object
#' 
#' @title Merge all computations efficiently into one object
#' @name merge_results
#' @param pb_data \code{list} or \code{data table} - Prediction band data, which must at least contain \code{comparison}, \code{predictor}, \code{prediction}, \code{lwr} and \code{upr}
#' @param ce_data \code{list} or \code{data table} - Commutability evaluation data for evaluated material, which must at least contain \code{comparison}, \code{SampleID}, \code{MP_B}, \code{MP_A}, \code{prediction}, \code{lwr}, \code{upr} and \code{inside}
#' @param zeta_data \code{list} or \code{data table} - Zeta estimates, which must at least contain \code{comparison}, \code{zeta}, \code{lwr}, \code{upr}, \code{zeta_critical}, \code{zeta_conclusion}
#' @param imprecision_data \code{list} or \code{data table} - global imprecision estimates, which must at least contain \code{comparison}, \code{CV_A}, \code{CV_A_lwr}, \code{CV_A_upr}, \code{CV_B}, \code{CV_B_lwr}, \code{CV_B_upr}, \code{lambda}, \code{lambda_lwr} and \code{lambda_upr}. This one may be omitted, if one is uninterested in viewing the imprecision estimates
#' @param rounding \code{integer} - How many decimals should be included in the float type variables? The implicit maximum is 6 decimals, meaning that any integer larger than 6 will produce a warning
#' @param include_imprecision_estimates \code{boolean} - Should imprecision estimates be part of the merged results. Default is \code{FALSE}
#' @param cv_percent \code{boolean} - Should CVs be given in percent or in standard decimal number. The latter is the default
#' 
#' @description Merge all the essential components (e.g., pb_data, ce_data, zeta_data) of the commutability evaluation analysis into one object, so it will be easier to plot and present commutability evaluation results
#' 
#' @details The merging is done with respect to the comparison column that needs to be common in every data input. The effects of missing values in the comparison variable may yield unexpected results or errors, so ensure that at least this column, is NA-free. 
#' 
#' @return A \code{list} with two main components, merged data results for commutability evaluation of control materials and merged results for prediction bands. The latter may be used to plot commutability evaluation plots
#'
#' @examples \dontrun{
#'   library(fasteqa)
#'   data <- simulate_data_eqa(list(n = 25, R = 3, cvx = 0.06, cvy = 0.04))
#'   estimate_zeta(data)
#' }
#'
NULL

merge_results <- function(pb_data, ce_data, zeta_data, imprecision_data, rounding = 3L, include_imprecision_estimates = TRUE, cv_percent = FALSE) {
    .Call(`_fasteqa_merge_results`, pb_data, ce_data, zeta_data, imprecision_data, rounding, include_imprecision_estimates, cv_percent)
}

#' Estimate prediction intervals for EQA data with Deming or OLS
#' 
#' @title Estimate prediction intervals for EQA data with Deming or OLS
#' @name predict_eqa
#' 
#' @param data \code{list} or \code{data table} - Mean-of-replicates clinical sample data with elements/columns \code{SampleID}, \code{MP_A} and \code{MP_B}
#' @param new_data \code{list} or \code{data table} - Mean-of-replicates control material data with \code{MP_B}. \code{SampleID} and \code{MP_A} may also be in new_data based on desired output structure:
#' \itemize{
#'   \item{\code{Minimum requirement: }}{Must contain \code{MP_B}}
#'   \item{\code{method = ols: }}{Must at least contain both \code{MP_B} and \code{MP_A}}
#'   \item{\code{Inside checks performed: }}{Must at least contain both \code{MP_B} and \code{MP_A}}
#'   \item{\code{Sample-wise prediction}}{Must at least contain \code{MP_B} and \code{SampleID} (plus \code{MP_A} if method = ols)}
#' }
#' @param imprecision_estimates \code{list} or \code{data table} - Imprecision estimates e.g., that which are outputted by \code{global_precision_estimates()}. Minimum requirement: \code{lambda} and \code{Var_B} must be part of the  
#' @param R integer - Number of replicates, which new_data is based on
#' @param method string - Which method should be used to estimate the prediction intervals. Default is \code{fg}. At the moment, possible values for method is
#' \itemize{
#'   \item{\code{fg: }}{Standard Deming regression, but prediction intervals are calculated using components from J. Gillards work}
#'   \item{\code{clsi: }}{Standard Deming regression, but prediction intervals are calculated based on derivation of Jeff Vaks}
#'   \item{\code{ols: }}{Ordinary least squares regression, but the predictor is chosen so that neglected prediction error variance is reduced}
#' }
#' @param level float - Confidence level of prediction intervals. Should ideally be corrected for simulations testing. Default level is 0.99
#' @param rounding integer - How many decimals should be included in the predictions and prediction intervals. Two or three decimals are often sufficient. Default is 3
#'
#' @description A rich function that calculates prediction intervals for EQA data
#' 
#'
#' @details If possible it is wise to always include \code{SampleID}, \code{MP_A} and \code{MP_B} in new_data. If by some reason only \code{MP_B} is available, one cannot use \code{method = ols}. For example, when construction predicton bands, we will only have \code{MP_B} is available.
#'
#' @return list - prediction interval data based on use-inputs
#'
#' @examples \dontrun{
#'   library(fasteqa)
#'   training_parameters <- list(n = 25, R = 3, cvx = 0.01, cvy = 0.015, cil = 10, ciu = 70)
#'   test_parameters <- list(n = 5, R = 3, cvx = 0.01, cvy = 0.015, cil = 10, ciu = 70)
#'   training_data <- simulate_eqa_data(training_parameters)
#'   test_data <- simulate_eqa_data(test_parameters)
#'   training_data$$SampleID <- as.character(training_data$SampleID)
#'   training_data$$ReplicateID <- as.character(training_data$ReplicateID)
#'   imprecision <- global_precision_estimates(data)
#'   mean_of_replicates_training_data <- fun_of_replicates(training_data)
#'   mean_of_replicates_test_data <- fun_of_replicates(test_data)
#'   prediction_intervals <- predict_eqa(mean_of_replicates_training data,
#'                                       mean_of_replicates_test_data,
#'                                       imprecision)
#' }
NULL

predict_eqa <- function(data, new_data, imprecision_estimates, R = 3L, method = "fg", level = 0.99, rounding = 3L) {
    .Call(`_fasteqa_predict_eqa`, data, new_data, imprecision_estimates, R, method, level, rounding)
}

#' Resample fun-of-replicates data
#' 
#' @title Resample fun-of-replicates data
#' @name resample_fun_data
#' 
#' @param data \code{list} or \code{data table} - Must contain \code{SampleID}, \code{MP_A} and \code{MP_B}. \code{SampleID} and \code{ReplicateID} must be of character type, or else an error is thrown
#' @param make_unique - \code{integer} that controls the output SampleID. If \code{make_unique = 1}, the default, new SampleIDs will be made. Conversely \code{make_unique = 0} will use the original SampleIDs. The latter is not recommended because potential calculations are affected by this 
#'
#' @description In order to construct bootstrap confidence intervals and do inference on a set of population parameters where the underlying distribution is complex, will require bootstrap replicates. This function is both efficient and does its job, but at a cost of a strict input requirement
#'
#' @details Combine with e.g., \code{predict_eqa()}, to estimate classification rates and more
#'
#' @return A list containing the resampled fun-of-replicates data. Use \code{setDT()} for maximum efficiency if you desire to convert the resampled data to a data table
#'
#' @examples \dontrun{
#'   library(fasteqa)
#'   data <- simulate_data_eqa(list(n = 25, R = 3, cvx = 0.06, cvy = 0.04))
#'   data$SampleID <- as.character(data$SampleID)
#'   data$ReplicateID <- as.character(data$ReplicateID)
#'   data <- fun_of_replicates(data, fun = "median")
#'   resampled_data <- resample_fun_data(data)
#' }
NULL

resample_fun_data <- function(data, make_unique = 1L) {
    .Call(`_fasteqa_resample_fun_data`, data, make_unique)
}

#' Resample clustered EQA clinical sample data
#' 
#' @title Resample clustered EQA clinical sample data
#' @name resample_samples
#' 
#' @param data A list or a data table with elements/columns \code{SampleID}, \code{ReplicateID}, \code{MP_A} and \code{MP_B}. \code{SampleID} and \code{ReplicateID} must be of character type for the function to operate correctly
#' @param silence An integer that controls the progress reports outputted for debugging and further examination of the command. \code{silence = -1} or \code{silence = 0} signify that progress reports should be printed. Default is \code{silence = 1} which suppresses all printing
#'
#' @description In order to construct bootstrap confidence intervals and do inference on a set of population parameters where the underlying distribution is complex, will require resample of clustered data. This function is both efficient and does its job, but at a cost of a strict input requirement
#'
#' @details \code{resample_samples()} is a very efficient algorithm to resample EQA. Combine with \code{Estimatek()} to resample k or combine with \code{CharacterEstimatePrecision()} to resample variability measures such as CVs and variances. May also be combined with other functions
#'
#' @return A list containing the resampled EQA clinical sample data. Use \code{setDT()} for maximum efficiency if needed to convert the resampled data to a data table
#'
#' @examples \dontrun{
#'   library(commutability.selectivity)
#'   data <- sdwdnsp2()
#'   data$SampleID <- as.character(data$SampleID)
#'   data$ReplicateID <- as.character(data$ReplicateID)
#'   resampled_data <- resample_samples(data)
#' }
NULL

resample_samples <- function(data, silence = 1L) {
    .Call(`_fasteqa_resample_samples`, data, silence)
}

#' Simulation of EQA data based on study design and potential differences in non-selectivity
#'
#' @title Simulation of EQA data based on study design and potential differences in non-selectivity
#' @name simulate_eqa_data
#' 
#' @param parameters A \code{list} of parameters used to simulate the EQA data. You must at least specify one parameter for this function to run. Except that one mandatory parameter, you may optionally choose the remaining of the parameters. These are the optimal parameters that you may include into the list:
#' \itemize{
#'   \item{\code{n: }}{  The number of clinical samples}
#'   \item{\code{R: }}{The number of replicates on each sample}
#'   \item{\code{cvx: }}{The analytical CV of x measurements}
#'   \item{\code{cvy: }}{The analytical CV of y measurements}
#'   \item{\code{cil: }}{The lower range of the concentration interval}
#'   \item{\code{ciu: }}{The upper range of the concentration interval}
#'   \item{\code{qpos: }}{Position of systematic differences in non-selectivity. 0 signify lower range and 1 signify upper range}
#'   \item{\code{qran: }}{Interquantile range where systematic differences in non-selectivity should have its effect}
#'   \item{\code{prop: }}{average proportion of clinical samples affected by random differences in non-selectivity}
#'   \item{\code{mmax: }}{The maximum relocation magnitude in number of analytical SDs of y measurements. This assumes either prop or qpos and qran to be specified as well}
#' }
#' @param silence \code{Integer} should temporary calculation results be printed? This may be useful for debugging or strange curiosity. \code{silence = 1} signify that printing will be suppressed, which is the default. \code{silence = 0} allows such printing 
#' 
#' @description Simulates a data set with n x R rows, and four columns. The two first columns are the base ID columns (\code{SampleID} and \code{ReplicateID}). The remaining columns are numeric columns holding measurement results from the two MSs in comparison (MS_A and MS_B) denoted \code{MP_A} and \code{MP_B}.
#' @details In order to convert the outputted list to a table, use either \code{as.data.frame()}, \code{as.data.table()}, \code{as.tibble()}. The most efficient way to convert is \code{setDT()} from the \code{data.table} package.
#'
#' @return A list where each list element is a column of the generated EQA data
#'
#' @examples \dontrun{
#'   simulated_data <- simulate_eqa_data(parameters = list(n = 25, R = 3, prop = 0.1, mmax = 5))
#'   simulated_data <- setDT(simulated_data)
#' }
#'
NULL

simulate_eqa_data <- function(parameters, silence = 1L) {
    .Call(`_fasteqa_simulate_eqa_data`, parameters, silence)
}

